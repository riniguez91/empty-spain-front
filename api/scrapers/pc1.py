""" Scrapper PC1

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1lF2VJGHtCZQltaX22iLq3zBAcMPuQEvg

# Actividad 5: Aplicacion Python Grupo ÍÑIGUEZ

## 1. Introducción y procesamiento previo de datos

Estamos cargando este conjunto de datos localmente, como un archivo Excel (xlsx), y agregando una columna que codifica la categoría como un número entero (las variables categóricas a menudo se representan mejor con números enteros que con cadenas).
"""
from elPais import * #importar el fichero elPais.py

def scrape_pc1():
    from numpy.core.records import array
    import pandas as pd

    from io import StringIO #Usa el dataset
    df = pd.read_excel('api/scrapers/data-pc1/Noticias_Excel.xlsx', engine='openpyxl')
    df['category_id'] = df['Category'].factorize()[0] #Se cambia la categoria 0-> despoblacion 1-> no despoblacion

    category_id_df = df[['Category', 'category_id']].drop_duplicates().sort_values('category_id') #quita los valores duplicados y ordena
    category_to_id = dict(category_id_df.values)
    id_to_category = dict(category_id_df[['category_id', 'Category']].values)

    """## 2. Exploración de datos

    Antes de profundizar en el entrenamiento de modelos de aprendizaje automático, debemos familiarizarnos con la estructura y las características de nuestro conjunto de datos.

    Primero mostraremos una serie de ejemplos aleatorios y su estructura:
    """

    list = open("api/scrapers/data-pc1/stopwords.txt").readlines() #Se lee el documento

    from sklearn.feature_extraction.text import TfidfVectorizer
    import nltk
    nltk.download('stopwords')

    from nltk.corpus import stopwords
    lista2 = [word for word in stopwords.words('spanish')]

    #TF-IDF lo vectoriza para saber la similitud de palabras, hace un podado de menos de 5 palabras no las tiene en cuenta, ngram junta palabras, stop words en español)
    tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='utf-8', ngram_range=(1, 2), stop_words=lista2)
    features = tfidf.fit_transform(df.Cuerpo).toarray()
    labels = df.category_id

    """## 3. Entrenamiento y evaluación del modelo

    ### **Métrica**

    Como estamos tratando con una tarea de clasificación de noticias y ya que se trata de un dataset equilibrado usaremos la métrica de precision (accuracy).

    ### **Elección de modelo**

    - **Logistic Regression**: un clasificador lineal, en su mayoría similar a la regresión lineal tradicional, pero que se ajusta a la salida de la función logística.

    - **(Multinomial) Naive Bayes**: Un modelo bayesiano, asumiendo total independencia entre características, que funciona sorprendentemente bien con el modelo Bag of Words, y se utilizaron principalmente para la detección de spam.

    - **Random Forest**: es el conjunto de una gran cantidad de árboles de decisión, cada uno entrenado en un subconjunto aleatorio de las características de entrada. Funcionan bien cuando se trata de relaciones de características complejas y son relativamente resistentes al sobreajuste.

    - **SVC**:  son un conjunto de métodos de aprendizaje supervisado que se utilizan para la clasificación, regresión y detección de valores atípicos. Las ventajas de SVM es que son eficaces en espacios de gran dimensión. Sigue siendo eficaz en los casos en que el número de dimensiones es mayor que el número de muestras.

    - **KNeighborsClassifier**: El algoritmo de K vecinos es un método de aprendizaje automático no paramétrico. Se utiliza para clasificación y regresión. En ambos casos, la entrada consta de los k ejemplos de entrenamiento más cercanos en el espacio de características.

    ### **Evaluación del modelo**

    Dividiremos el dataset en dos conjuntos:

    - Un conjunto de entrenamiento en el que se entrenará el modelo.
    - Un conjunto de validación para obtener la precision del modelo.

    Para evaluar cada modelo, usaremos la técnica de validación cruzada K-fold, con 10 folds de forma que obtenemos un 10% (1 fold) de los datos para validacion y un 90% (9 folds) de los datos para el entrenamiento: entrenando iterativamente el modelo en diferentes subconjuntos de datos y probando contra los datos obtenidos para evitar problemas comunes como "overfitting" (el modelo obtiene buenos resultados en el conjunto de entrenamiento pero obtiene resultados pobres en el conjunto de validacion) y "underfitting" (el modelo obtiene resultados muy pobres en el conjunto de validacion y por lo general también en el conjunto de entrenamiento).
    """

    from sklearn.linear_model import LogisticRegression
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.naive_bayes import MultinomialNB
    from sklearn.svm import SVC
    from sklearn.neighbors import KNeighborsClassifier

    from sklearn.model_selection import cross_val_score

    models = [
        RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),
        MultinomialNB(),
        LogisticRegression(random_state=0),
        SVC(),
        KNeighborsClassifier(),
    ]
    CV = 10 #5
    cv_df = pd.DataFrame(index=range(CV * len(models)))
    entries = []
    for model in models:
      model_name = model.__class__.__name__
      accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)
      for fold_idx, accuracy in enumerate(accuracies):
        entries.append((model_name, fold_idx, accuracy))
    cv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])

    """### 4. Interpretación del modelo

    Profundizando en la interpretacion del modelo observamos tambien la matriz de confusion para ver cuantos falsos verdaderos (entre otras cosas) predice nuestro modelo donde cada columna de la matriz representa el número de predicciones de cada clase, mientras que cada fila representa a los casos verdadores de la clase.
    """

    from sklearn.model_selection import train_test_split
    import pickle

    model = LogisticRegression(random_state=0)

    X_train, X_test, y_train, y_test, indices_train, indices_test = train_test_split(features, labels, df.index, test_size=0.33, random_state=0)
    model.fit(X_train, y_train)
    y_pred_proba = model.predict_proba(X_test)
    y_pred = model.predict(X_test)
 
    """Analizamos en mas detalle los falsos verdaderos para la clase Despoblacion para comprobar que es lo que esta causando estos fallos:

    Como se puede ver anteriormente, los artículos clasificados erróneamente son artículos que hablan de diferentes temas (artículos que involucran palabras relacionadas con la Despoblacion, como aquellos que hablan de pueblos de España). Cabe destacar que un modelo con una precision máxima 100% es un caso poco realista ya que siempre existen fallos en el modelo debido a esto.

    Posteriormente comprobamos que terminos se asocian más frecuentemente con las distintas clases (Despoblacion y No Despoblacion) usando la técnica chi cuadrado pora obtener los unigramas y bigramas:
    """

    model.fit(features, labels)

    # Save the model & tf-idf so that we can use it later
    pickle.dump(model, open('api/scrapers/data-pc1/trained_model.sav', 'wb'))
    pickle.dump(tfidf, open('api/scrapers/data-pc1/tfidf.pkl', 'wb'))

scrape_pc1()

